{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zBETH7w-t_U"
      },
      "source": [
        "## The Transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:\n",
        "- Create a Transformer encoder, one of the basic components of the Transformer\n",
        "architecture.\n",
        "- Apply it to the IMDB movie review classification task"
      ],
      "metadata": {
        "id": "k4B6JBH0VmSM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rnciTC7-t_W"
      },
      "source": [
        "### Understanding self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Self-attention** - a smart embedding space would ***provide a different vector representation for a word depending on the other words surrounding it***.\n",
        "- The **purpose of self-attention** is to ***modulate(adjust or regulate)*** the representation of a token by using the representations of related tokens in the sequence. This produces ***context-aware token representations.***"
      ],
      "metadata": {
        "id": "IRGYFE973lpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy-like pseudocode:"
      ],
      "metadata": {
        "id": "lX1JFCekkpDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def self_attention(input_sequence):\n",
        "  output = np.zeros(shape=input_sequence.shape)\n",
        "  # Iterate over each token in the input sequence.\n",
        "  for i, pivot_vector in enumerate(input_sequence):\n",
        "    scores = np.zeros(shape=(len(input_sequence),))\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      # Compute the dot product (attention score) between the token and every other token.\n",
        "      scores[j] = np.dot(pivot_vector, vector.T)\n",
        "      # Scale by a normalization factor, and apply a softmax.\n",
        "      scores /= np.sqrt(input_sequence.shape[1])\n",
        "      scores = softmax(scores)\n",
        "      new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      # Take the sum of all tokens weighted by the attention scores.\n",
        "      new_pivot_representation += vector * scores[j]\n",
        "      # That sum is our output.\n",
        "      output[i] = new_pivot_representation\n",
        "  return output"
      ],
      "metadata": {
        "id": "4TGmBK2sVcvi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZVWF71-t_W"
      },
      "source": [
        "#### Generalized self-attention: the query-key-value model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the general case, you could be doing this with three different sequences. We’ll call them “***query***,” “***keys***,” and “***values***.”\n",
        "    - The operation becomes “for each element in the query, compute how much the element is related to every key, and use these scores to weight a sum of values”:\n",
        "  \n",
        "      ```\n",
        "      outputs = sum(inputs * pairwise_score(inputs, inputs))\n",
        "      ```  \n",
        "- Conceptually, this is what Transformer-style attention is doing.\n",
        "    - You’ve got a reference sequence that describes something you’re looking for: ***the query***.\n",
        "    - You’ve got a body of knowledge that you’re trying to extract information from: ***the values***.\n",
        "    - ***Each value is assigned a key that describes the value*** in a format that can be readily compared to a query.\n",
        "\n",
        "      ```\n",
        "      outputs = sum(values * pairwise_score(query, keys))\n",
        "      ```  "
      ],
      "metadata": {
        "id": "shmsao4lHLRm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBfnH3Pg-t_X"
      },
      "source": [
        "### Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **What are these “multiple heads” referred to?**\n",
        "- “***Multi-head attention***” is an extra tweak to the self-attention mechanism, introduced\n",
        "in “Attention is all you need.”\n",
        "    - The “***multi-head***” moniker refers to the fact that the ***output space of the self attention layer gets factored into a set of independent subspaces***,\n",
        "    - ***learned separately***: the initial ***query***, ***key***, and ***value*** are ***sent through three\n",
        "    independent sets of dense projections***, ***resulting in three separate vectors***.\n",
        "    - Each vector is processed via neural attention,\n",
        "    - and the ***three outputs are concatenated back together*** into a single output sequence.\n",
        "    - Each such subspace is called a “head.”"
      ],
      "metadata": {
        "id": "qrFGPBrnI5kW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhyrY0yF-t_X"
      },
      "source": [
        "### The Transformer encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWr4Nx1n-t_Y"
      },
      "source": [
        "**Getting the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D2SQ0Yti-t_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851f3bb4-5cb3-47e6-f323-5be505654f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  47.5M      0  0:00:01  0:00:01 --:--:-- 47.6M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HzwuWBB-t_a"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ATGMVcF-t_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5f3017-ee40-483b-882f-a49bc4b3fa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ro3FRc7-t_c"
      },
      "source": [
        "**Vectorizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UO7vG08K-t_d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyO08sQ-t_e"
      },
      "source": [
        "**Transformer encoder implemented as a subclassed `Layer`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BatchNormalization** doesn’t work well for sequence data.\n",
        "- Using the **LayerNormalization** layer, which normalizes each sequence independently from other sequences in the batch.\n",
        "\n",
        "While ***BatchNormalization collects information from many samples*** to obtain accurate statistics for the feature means and variances, ***LayerNormalization pools data within each sequence separately***, which is more appropriate for sequence data."
      ],
      "metadata": {
        "id": "cq-IBGajlzzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2XQy3Ugs-t_f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    # initialize variables\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # size of the input token vector - embedding token vector representation\n",
        "        self.embed_dim = embed_dim\n",
        "        # size of the inner dense layer - use for dense projection\n",
        "        self.dense_dim = dense_dim\n",
        "        # number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # initialize multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # dense projection - independently learned linear projections\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # layer normalization - help gradients flow better during backpropagation\n",
        "        # normalizes each sequence independently from other sequences in the batch\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # computation goes in call()\n",
        "    # the call() method is called automatically when the layer is used in a Keras model\n",
        "    def call(self, inputs, mask=None):\n",
        "        # The mask that will be generated by the Embedding layer will be 2D,\n",
        "        # but the attention layer expects to be 3D or 4D, so we expand its rank\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        # Save attention_output as Residual connection\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    # implement serialization so we can save the model\n",
        "    # get_config method: this enables the layer to be reinstantiated from its config dict,\n",
        "    # which is useful during model saving and loading.\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KebcsW2k-t_f"
      },
      "source": [
        "**Using the Transformer encoder for text classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J-Ju2StT-t_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5768169c-26ac-4938-cfc3-ae58563c0675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Reason to apply GlobalMaxPooling1D layer:\n",
        "# Since TransformerEncoder returns full sequences, we need to reduce each\n",
        "# sequence to a single vector for classification via a global pooling layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToGGaiok-t_g"
      },
      "source": [
        "**Training and evaluating the Transformer encoder based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew6R6bsJ-t_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "151428f8-b3c7-4ae9-f923-c932d5aa0cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 3193s 5s/step - loss: 0.5052 - accuracy: 0.7628 - val_loss: 0.3460 - val_accuracy: 0.8468\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 3124s 5s/step - loss: 0.3381 - accuracy: 0.8521 - val_loss: 0.3208 - val_accuracy: 0.8602\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 3107s 5s/step - loss: 0.3008 - accuracy: 0.8719 - val_loss: 0.2999 - val_accuracy: 0.8694\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 3046s 5s/step - loss: 0.2701 - accuracy: 0.8888 - val_loss: 0.3147 - val_accuracy: 0.8658\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 2954s 5s/step - loss: 0.2432 - accuracy: 0.9028 - val_loss: 0.3043 - val_accuracy: 0.8720\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 3045s 5s/step - loss: 0.2138 - accuracy: 0.9145 - val_loss: 0.2860 - val_accuracy: 0.8790\n",
            "Epoch 7/20\n",
            "458/625 [====================>.........] - ETA: 12:37 - loss: 0.1867 - accuracy: 0.9273"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dbc18b26598d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     save_best_only=True)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m model = keras.models.load_model(\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"transformer_encoder.keras\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The model has trained very slowly which is interrupted by the GPU limited time during computing"
      ],
      "metadata": {
        "id": "YKHsnoXGI-Kb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cDOEt39-t_g"
      },
      "source": [
        "#### Using positional encoding to re-inject order information"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The idea behind ***positional encoding*** is very simple: ***to give the model access to word order information*** ***by adding the word’s position in the sentence to each word embedding***.\n",
        "- The ***input word embeddings*** will have two components:\n",
        "    - the usual ***word vector***, which ***represents the word independently*** of any specific context,\n",
        "    - and a ***position vector***, which ***represents the position of the word in the current sentence**.*\n",
        "    \n",
        "    The model will then figure out how to best leverage this additional information\n",
        "    \n",
        "- **Positional embedding**: The technique used to proceed to ***add position embeddings to the corresponding word embeddings to obtain position-aware word embedding***"
      ],
      "metadata": {
        "id": "8KuRreViA0bU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FTRSMMA-t_h"
      },
      "source": [
        "**Implementing positional embedding as a subclassed layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Positional embedding**: The technique used to proceed to ***add position embeddings to the corresponding word embeddings to obtain position-aware word embedding***"
      ],
      "metadata": {
        "id": "q7Oi_0SeUKhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qK52x3Ya-t_h"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    # A downside of position embeddings is that the sequence length needs to be known in advance\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Prepare an Embedding layer for the token indices\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        # And another one for the token positions\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        # Add both embedding vectors together\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    # Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs.\n",
        "    # The compute_mask method will called automatically by the framework, and the mask will get propagated to the next layer\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    # Implement serialization so we can save the model\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-OwgcH9-t_h"
      },
      "source": [
        "#### Putting it all together: A text-classification Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLBkXXjo-t_i"
      },
      "source": [
        "**Combining the Transformer encoder with positional embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "osanYuTu-t_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c63986-1a5f-422e-c7e3-1fe7aff0084c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         5273600   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5817633 (22.19 MB)\n",
            "Trainable params: 5817633 (22.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "625/625 [==============================] - 61s 89ms/step - loss: 0.5243 - accuracy: 0.7395 - val_loss: 0.5221 - val_accuracy: 0.7738\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 50s 79ms/step - loss: 0.3025 - accuracy: 0.8720 - val_loss: 0.3204 - val_accuracy: 0.8618\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 48s 76ms/step - loss: 0.2365 - accuracy: 0.9049 - val_loss: 0.2818 - val_accuracy: 0.8870\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 47s 76ms/step - loss: 0.1924 - accuracy: 0.9232 - val_loss: 0.2927 - val_accuracy: 0.8878\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 46s 74ms/step - loss: 0.1587 - accuracy: 0.9402 - val_loss: 0.3169 - val_accuracy: 0.8880\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 47s 74ms/step - loss: 0.1362 - accuracy: 0.9493 - val_loss: 0.3470 - val_accuracy: 0.8838\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 46s 74ms/step - loss: 0.1164 - accuracy: 0.9569 - val_loss: 0.3457 - val_accuracy: 0.8858\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0934 - accuracy: 0.9661 - val_loss: 0.4245 - val_accuracy: 0.8846\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0762 - accuracy: 0.9728 - val_loss: 0.4469 - val_accuracy: 0.8786\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0586 - accuracy: 0.9791 - val_loss: 0.4750 - val_accuracy: 0.8758\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0447 - accuracy: 0.9852 - val_loss: 0.6564 - val_accuracy: 0.8752\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 0.7129 - val_accuracy: 0.8718\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0286 - accuracy: 0.9900 - val_loss: 0.9010 - val_accuracy: 0.8492\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0240 - accuracy: 0.9916 - val_loss: 0.7701 - val_accuracy: 0.8732\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 1.0847 - val_accuracy: 0.8722\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0194 - accuracy: 0.9938 - val_loss: 0.9629 - val_accuracy: 0.8710\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 1.0683 - val_accuracy: 0.8738\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.9827 - val_accuracy: 0.8730\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0123 - accuracy: 0.9965 - val_loss: 1.1265 - val_accuracy: 0.8728\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 1.3143 - val_accuracy: 0.8660\n",
            "782/782 [==============================] - 23s 28ms/step - loss: 0.2975 - accuracy: 0.8740\n",
            "Test acc: 0.874\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds,\n",
        "          validation_data=int_val_ds,\n",
        "          epochs=20,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We acheive 87.4% test accuracy."
      ],
      "metadata": {
        "id": "rRISrSkSGEhm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}